{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1134f74d",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed8581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0814fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8987a78c",
   "metadata": {},
   "source": [
    "# Reading in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c00b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1882931173</td>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940636800</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "      <td>This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>Kevin Killian</td>\n",
       "      <td>10/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1095724800</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "      <td>I don't care much for Dr. Seuss but after reading Philip Nel's book I changed my mind--that's a good testimonial to the power of Rel's writing and thinking. Rel plays Dr. Seuss the ultimate compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>John Granger</td>\n",
       "      <td>10/11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1078790400</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "      <td>If people become the books they read and if \"the child is father to the man,\" then Dr. Seuss (Theodor Seuss Geisel) is the most influential author, poet, and artist of modern times. For me, a dadd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1090713600</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;Dr. Seuss,&amp;quot; was one of the most influential writers and artists of the 20th century.In 1959, Rudolf Flesch wrote, &amp;quot;A hundred years from now, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                           Title  Price         User_id  \\\n",
       "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
       "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
       "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
       "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
       "\n",
       "                          profileName review/helpfulness  review/score  \\\n",
       "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
       "1                       Kevin Killian              10/10           5.0   \n",
       "2                        John Granger              10/11           5.0   \n",
       "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
       "\n",
       "   review/time                                   review/summary  \\\n",
       "0    940636800           Nice collection of Julie Strain images   \n",
       "1   1095724800                                Really Enjoyed It   \n",
       "2   1078790400  Essential for every personal and Public Library   \n",
       "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
       "\n",
       "                                                                                                                                                                                               review/text  \n",
       "0  This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the ...  \n",
       "1  I don't care much for Dr. Seuss but after reading Philip Nel's book I changed my mind--that's a good testimonial to the power of Rel's writing and thinking. Rel plays Dr. Seuss the ultimate compli...  \n",
       "2  If people become the books they read and if \"the child is father to the man,\" then Dr. Seuss (Theodor Seuss Geisel) is the most influential author, poet, and artist of modern times. For me, a dadd...  \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;Dr. Seuss,&quot; was one of the most influential writers and artists of the 20th century.In 1959, Rudolf Flesch wrote, &quot;A hundred years from now, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Books_rating.csv')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f919b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df_1 = df.copy()\n",
    "df_2 = df_1[[\"review/text\",\"review/summary\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b64fe2",
   "metadata": {},
   "source": [
    "## Data info and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2e4b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   review/text     object\n",
      " 1   review/summary  object\n",
      "dtypes: object(2)\n",
      "memory usage: 45.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9d7e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "      <th>review/summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2999992</td>\n",
       "      <td>2999962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2062648</td>\n",
       "      <td>1592315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>digital books are perfect and easy to use! They take up no space on the bookshelf and are always with you!!!</td>\n",
       "      <td>Great Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>322</td>\n",
       "      <td>6848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         review/text  \\\n",
       "count                                                                                                        2999992   \n",
       "unique                                                                                                       2062648   \n",
       "top     digital books are perfect and easy to use! They take up no space on the bookshelf and are always with you!!!   \n",
       "freq                                                                                                             322   \n",
       "\n",
       "       review/summary  \n",
       "count         2999962  \n",
       "unique        1592315  \n",
       "top        Great Book  \n",
       "freq             6848  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6661e3e",
   "metadata": {},
   "source": [
    "# Renaming the two necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763fcbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the ...</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after reading Philip Nel's book I changed my mind--that's a good testimonial to the power of Rel's writing and thinking. Rel plays Dr. Seuss the ultimate compli...</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"the child is father to the man,\" then Dr. Seuss (Theodor Seuss Geisel) is the most influential author, poet, and artist of modern times. For me, a dadd...</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;Dr. Seuss,&amp;quot; was one of the most influential writers and artists of the 20th century.In 1959, Rudolf Flesch wrote, &amp;quot;A hundred years from now, ...</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is basically an academic overview of Seuss poetry, art, cartoons, and the problems with the commercialization of the Seuss name and works after his death....</td>\n",
       "      <td>Good academic overview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>This is an extremely difficult book to digest, and it is not for casual readers. However, Collingwood's ideas on a meeting of minds between past and present is absolutely fascinating and gets to t...</td>\n",
       "      <td>Difficult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>This is pretty interesting. Collingwood seems like on of the first historians to really utilize ideas from evolutionary theory and modern psychology in his overall method. He manages to create a v...</td>\n",
       "      <td>Quite good and ahead of its time occasionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>This is a good book but very esoteric. \"What is History?\" by E.H. Carr is an easier selection for the causal reader or someone beginning to study historiography.</td>\n",
       "      <td>Easier reads of those not well versed in historiography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>My daughter, a freshman at Indiana University, e-mailed me a list of the books she needed. This was on it... I ordered it, paid for it, and had it shipped directly to her. It arrived sooner than e...</td>\n",
       "      <td>Yes, it is cheaper than the University Bookstore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>The guy has a few good ideas but, reader, beware; this is the transciption of several different lectures over several years. I use the term &amp;quot;different&amp;quot; loosely because each appears to be...</td>\n",
       "      <td>Collingwood's ideas sink in a quagmire or verbiage.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                            text  \\\n",
       "0        This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the ...   \n",
       "1        I don't care much for Dr. Seuss but after reading Philip Nel's book I changed my mind--that's a good testimonial to the power of Rel's writing and thinking. Rel plays Dr. Seuss the ultimate compli...   \n",
       "2        If people become the books they read and if \"the child is father to the man,\" then Dr. Seuss (Theodor Seuss Geisel) is the most influential author, poet, and artist of modern times. For me, a dadd...   \n",
       "3        Theodore Seuss Geisel (1904-1991), aka &quot;Dr. Seuss,&quot; was one of the most influential writers and artists of the 20th century.In 1959, Rudolf Flesch wrote, &quot;A hundred years from now, ...   \n",
       "4        Philip Nel - Dr. Seuss: American IconThis is basically an academic overview of Seuss poetry, art, cartoons, and the problems with the commercialization of the Seuss name and works after his death....   \n",
       "...                                                                                                                                                                                                          ...   \n",
       "2999995  This is an extremely difficult book to digest, and it is not for casual readers. However, Collingwood's ideas on a meeting of minds between past and present is absolutely fascinating and gets to t...   \n",
       "2999996  This is pretty interesting. Collingwood seems like on of the first historians to really utilize ideas from evolutionary theory and modern psychology in his overall method. He manages to create a v...   \n",
       "2999997                                        This is a good book but very esoteric. \"What is History?\" by E.H. Carr is an easier selection for the causal reader or someone beginning to study historiography.   \n",
       "2999998  My daughter, a freshman at Indiana University, e-mailed me a list of the books she needed. This was on it... I ordered it, paid for it, and had it shipped directly to her. It arrived sooner than e...   \n",
       "2999999  The guy has a few good ideas but, reader, beware; this is the transciption of several different lectures over several years. I use the term &quot;different&quot; loosely because each appears to be...   \n",
       "\n",
       "                                                         summary  \n",
       "0                         Nice collection of Julie Strain images  \n",
       "1                                              Really Enjoyed It  \n",
       "2                Essential for every personal and Public Library  \n",
       "3                Phlip Nel gives silly Seuss a serious treatment  \n",
       "4                                         Good academic overview  \n",
       "...                                                          ...  \n",
       "2999995                                                Difficult  \n",
       "2999996            Quite good and ahead of its time occasionally  \n",
       "2999997  Easier reads of those not well versed in historiography  \n",
       "2999998         Yes, it is cheaper than the University Bookstore  \n",
       "2999999      Collingwood's ideas sink in a quagmire or verbiage.  \n",
       "\n",
       "[3000000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.rename(columns={\"review/text\":\"text\", \"review/summary\":\"summary\"}, inplace=True)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7758e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2.dropna(how='any', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3efd6d2",
   "metadata": {},
   "source": [
    "# Limiting the rows of the data to 3000\n",
    "\n",
    "### This eradicates longer runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20865ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2[:100000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a41a53",
   "metadata": {},
   "source": [
    "# Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb6e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets nltk rouge_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f27e62f",
   "metadata": {},
   "source": [
    "# Rouge score calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "278d7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: round(value.mid.fmeasure * 100, 1) for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec64bdbc",
   "metadata": {},
   "source": [
    "# creating a BASELINE Model and calculating the Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc587694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1 senctences: Scores {'rouge1': 10.8, 'rouge2': 3.1, 'rougeL': 9.9, 'rougeLsum': 9.9}\n",
      "First 2 senctences: Scores {'rouge1': 9.0, 'rouge2': 2.4, 'rougeL': 8.0, 'rougeLsum': 8.0}\n",
      "First 3 senctences: Scores {'rouge1': 7.7, 'rouge2': 2.0, 'rougeL': 6.7, 'rougeLsum': 6.7}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ref_summaries = list(df_3['summary'])\n",
    "\n",
    "for i in range (3):\n",
    "    candidate_summaries = list(df_3['text'].apply(lambda x: ' '.join(re.split(r'(?<=[.:;])\\s', x)[:i+1])))\n",
    "    print(f\"First {i+1} senctences: Scores {calc_rouge_scores(candidate_summaries, ref_summaries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd9d4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Your max_length is set to 200, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"this is only for Julie Strain fans . there's only about 2 pages with text and everything else is photos . if you like Julie like I like Julie, you won't go wrong .\"}]\n",
      "t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "print(summarizer(df_3['text'][0]))\n",
    "print(summarizer.model.config.__getattribute__('_name_or_path'))\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475f0ffe",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "###     - Removing extra white spaces\n",
    "###     - Expand contractions\n",
    "###     - Remove special case characters\n",
    "###     - Lowercasing all letters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a9ead0c",
   "metadata": {},
   "source": [
    "### Dictionary to be used for expanding contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4be97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "040a5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in df_3['text']:\n",
    "    cleaned_text.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b44d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for summary\n",
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString\n",
    "\n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in df_3['summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "df_3['cleaned_text']=cleaned_text\n",
    "df_3['cleaned_summary']=cleaned_summary\n",
    "df_3['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "df_3.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78b1b799",
   "metadata": {},
   "source": [
    "### Adding tokens to the start and end of the summary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30356cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['cleaned_summary'] = df_3['cleaned_summary'].apply(lambda x : '_START_ '+ x + ' _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70f8d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: julie strain fans collection photos pages worth nice section paintings olivia looking heavy literary content place find pages text everything else photos bottom line want one book six foot one probably better choice however like julie like like julie wrong one either\n",
      "Summary: _START_ sostoknice collection of julie strain imageseostok  _END_\n",
      "\n",
      "\n",
      "Review: care much seuss reading philip nel book changed mind good testimonial power rel writing thinking rel plays seuss ultimate compliment treating serious poet well one century interesting visual artists reading book decided trip mandeville collections library university california san diego order could visit incredible seuss geisel holdings almost much take like william butler yeats seuss led career constantly shifted metamoprhized meet new historical political cirsumstances seems leftist conservative different junctures career politics art nel shows cartoonist fabled magazine like andy warhol served time slaving business service amusing broadening minds children nel hesitate administer sound spanking seuss industry since death seen fit license kinds awful products including recent cat hat film mike myers cat astrophe book great especially recommend work picture editor given bounty good illustrations\n",
      "Summary: _START_ sostokreally enjoyed iteostok  _END_\n",
      "\n",
      "\n",
      "Review: people become books read child father man seuss influential author poet artist modern times daddy large family learned read seuss memorized many books via repeated readings young children prof nel brilliant american icon long awaited treat last serious treatment remarkable genius engaging read filled remarkable insights especially enjoyed prof nel discussions disneyfication seuss nel links failings american copyright law sides seuss sides new political genesis secular morality wwii cartoon work magazine chapters geisel poetry artwork link nel makes seuss historical avant guarde alone make book must buy parents serious readers mention public libraries readers nel books find engaging writing style makes book fun read imparting mountain information important ideas simply best comprehensive book yet written work seuss geisel certainly standard many years come thank prof nel wherever reader grew good doctor growing years later book written encyclopeadic knowledge children literature media genre scanning verse cubist painting explains power limits popularity seuss phenomenon\n",
      "Summary: _START_ sostokessential for every personal and public libraryeostok  _END_\n",
      "\n",
      "\n",
      "Review: theodore seuss geisel aka quot seuss quot one influential writers artists century rudolf flesch wrote quot hundred years children parents still eagerly read books fellow called ted geisel popularly known seuss quot flesch conservative prediction century today seuss still read many authors today bestseller lists forgotten published centenary geisel birth seuss american icon analyzes six key aspects seuss career poetry politics art biography marketing influence six insightful chapters philip nel assistant professor english kansas state university discusses quot laureate nonsense quot quot seuss adolf hitler quot quot doc smock quot quot fingers quot quot disneyfication seuss quot quot cat hat president quot nel gives short shrift geisel childhood family background indeed biography general preferring focus seuss writing art first book think saw mulberry street last book places seuss breakthrough year published two books often identified cat hat grinch stole christmas classic works seussian canon horton hears yertle turtle green eggs ham sneeches lorax butter battle book favorite work among books authored cat hat taught children read many books clear powerful moral seuss horror heavy handed preaching sought teach ignite imagination lifelong opponent smug self righteous bourgeois moralism quot seuss contrarian quot writes nel quot enjoyed challenging people reconsider assumptions rebellious imagination dispositional distaste rules regulations quot work quot rational insanity quot exhibited quot joyous anarchy quot quot lifelong thrill misbehaving quot better subtitle nel work would american icon iconoclast nel tells seuss early years advertising artist agitprop cartoonist book however biography serious study genres literary art criticism readers want biographical information nel recommends seuss geisel judith morgan neil morgan describes quot definitive biography single best secondary source seuss discussion seuss life work must begin book quot seuss american icon includes pages notes index comprehensive annotated seuss bibliography ever assembled one learns lot book author lucid style makes enlightening work fun read philip nel author rowling harry potter novels reader guide avant garde american postmodernity roy perry nolensville tennessee advertising copywriter nashville publishing house\n",
      "Summary: _START_ sostokphlip nel gives silly seuss serious treatmenteostok  _END_\n",
      "\n",
      "\n",
      "Review: philip nel seuss american iconthis basically academic overview seuss poetry art cartoons problems commercialization seuss name works death real extent biography seeking move academic book leans dry side assumes reader fairly good knowledge children literature century cartoons book begin seuss experience read children interested writing style art style interested deconstruction recent rush cash seuss hollywood advertisers think nel wants come based seuss background projects approved lifetime tough argument make end though nel point maybe movies tie ins crass book well researched lots neat tidbits gleamed early cartoons seuss magazine occasionally shockingly racist makes little human puts latter works like lorax new light education may enjoy background fans seuss enjoy exhaustive bibliography seuss many many works also good list works man\n",
      "Summary: _START_ sostokgood academic overvieweostok  _END_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",df_3['cleaned_text'][i])\n",
    "    print(\"Summary:\",df_3['cleaned_summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c18b14ac",
   "metadata": {},
   "source": [
    "# Getting the percentile values of the word count of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc362cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile value is 1\n",
      "10 percentile value is 27\n",
      "20 percentile value is 41\n",
      "30 percentile value is 57\n",
      "40 percentile value is 74\n",
      "50 percentile value is 95\n",
      "60 percentile value is 121\n",
      "70 percentile value is 157\n",
      "80 percentile value is 213\n",
      "90 percentile value is 323\n",
      "100 percentile value is  4928\n",
      " FROM 90 TO 100\n",
      "90 percentile value is 323\n",
      "91 percentile value is 341\n",
      "92 percentile value is 363\n",
      "93 percentile value is 389\n",
      "94 percentile value is 419\n",
      "95 percentile value is 455\n",
      "96 percentile value is 499\n",
      "97 percentile value is 554\n",
      "98 percentile value is 640\n",
      "99 percentile value is 793\n",
      "100 percentile value is  4928\n"
     ]
    }
   ],
   "source": [
    "df_3['word count text'] = df_3['text'].apply(lambda x : len(str(x).split()))\n",
    "for i in range(0,100,10):\n",
    "    var = df_3['word count text'].values\n",
    "    var = np.sort(var, axis=None)\n",
    "    print('{} percentile value is {}'.format(i, var[int(len(var)*(float(i)/100))]))\n",
    "print('100 percentile value is ', var[-1])\n",
    "\n",
    "\n",
    "\n",
    "print(' FROM 90 TO 100')\n",
    "\n",
    "\n",
    "for i in range(90,100):\n",
    "    var = df_3['word count text'].values\n",
    "    var = np.sort(var, axis=None)\n",
    "    print('{} percentile value is {}'.format(i, var[int(len(var)*(float(i)/100))]))\n",
    "print('100 percentile value is ', var[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5b728d6",
   "metadata": {},
   "source": [
    "### For summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6432e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile value is 1\n",
      "10 percentile value is 2\n",
      "20 percentile value is 2\n",
      "30 percentile value is 3\n",
      "40 percentile value is 4\n",
      "50 percentile value is 4\n",
      "60 percentile value is 5\n",
      "70 percentile value is 6\n",
      "80 percentile value is 7\n",
      "90 percentile value is 9\n",
      "100 percentile value is  29\n",
      " FROM 90 TO 100\n",
      "90 percentile value is 9\n",
      "91 percentile value is 9\n",
      "92 percentile value is 9\n",
      "93 percentile value is 9\n",
      "94 percentile value is 10\n",
      "95 percentile value is 10\n",
      "96 percentile value is 10\n",
      "97 percentile value is 11\n",
      "98 percentile value is 12\n",
      "99 percentile value is 13\n",
      "100 percentile value is  29\n"
     ]
    }
   ],
   "source": [
    "df_3['word count summary'] = df_3['summary'].apply(lambda x : len(str(x).split()))\n",
    "for i in range(0,100,10):\n",
    "    var = df_3['word count summary'].values\n",
    "    var = np.sort(var, axis=None)\n",
    "    print('{} percentile value is {}'.format(i, var[int(len(var)*(float(i)/100))]))\n",
    "print('100 percentile value is ', var[-1])\n",
    "\n",
    "\n",
    "\n",
    "print(' FROM 90 TO 100')\n",
    "\n",
    "\n",
    "for i in range(90,100):\n",
    "    var = df_3['word count summary'].values\n",
    "    var = np.sort(var, axis=None)\n",
    "    print('{} percentile value is {}'.format(i, var[int(len(var)*(float(i)/100))]))\n",
    "print('100 percentile value is ', var[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f35640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum for both text and summary\n",
    "\n",
    "max_count_text = 1500\n",
    "max_count_summary = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92310e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset with train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_3['text'], df_3['summary'], test_size=0.3, random_state=23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d11064ad",
   "metadata": {},
   "source": [
    "## Using the GloVe embedding \n",
    "\n",
    "#### Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f44b0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "all the words in the corpus 10266079\n",
      "the unique words in the corpus 409026\n",
      "The number of words that are present in both glove vectors and our corpus are 52381 which is nearly 13.0% \n",
      "word 2 vec length 52381\n"
     ]
    }
   ],
   "source": [
    "#Loading our Glove Model \n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Calculating Percentage of words from train text present in Word2vec model\n",
    "words_source_train = []\n",
    "for i in x_train :\n",
    "  words_source_train.extend(i.split(' '))\n",
    "## Find the total number of words in the Train data of Essays.\n",
    "print(\"all the words in the corpus\", len(words_source_train))\n",
    "## Find the unique words in this set of words\n",
    "words_source_train = set(words_source_train)\n",
    "print(\"the unique words in the corpus\", len(words_source_train))\n",
    "## Find the words present in both Glove Vectors as well as our corpus.\n",
    "inter_words = set(embeddings_index.keys()).intersection(words_source_train)\n",
    "print(\"The number of words that are present in both glove vectors and our corpus are {} which \\\n",
    "is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))/len(words_source_train))\n",
    "*100)))\n",
    "words_corpus_source_train = {}\n",
    "words_glove = set(embeddings_index.keys())\n",
    "for i in words_source_train:\n",
    "  if i in words_glove:\n",
    "    words_corpus_source_train[i] = embeddings_index[i]\n",
    "print(\"word 2 vec length\", len(words_corpus_source_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aecdb67",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "#### _Changing_ and _Passing_ the text values into a more understandable format for the model\n",
    "#### Tokenizing both _text_ and _summary_ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf15b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cbb279f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_index \u001b[39m=\u001b[39m x_tokenizer\u001b[39m.\u001b[39mword_index\n\u001b[0;32m      2\u001b[0m embedding_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(word_index) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m word, i \u001b[39min\u001b[39;00m word_index\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "word_index = x_tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
